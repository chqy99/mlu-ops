/*************************************************************************
 * Copyright (C) [2022] by Cambricon, Inc.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the
 * "Software"), to deal in the Software without restriction, including
 * without limitation the rights to use, copy, modify, merge, publish,
 * distribute, sublicense, and/or sell copies of the Software, and to
 * permit persons to whom the Software is furnished to do so, subject to
 * the following conditions:
 *
 * The above copyright notice and this permission notice shall be included
 * in all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
 * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
 * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 *************************************************************************/
#include "add.h"
#include "kernels/kernel.h"

#ifndef PAD_DOWN
#define PAD_DOWN(x, y) (((x) / (y)) * (y))
#endif

#ifndef PAD_UP
#define PAD_UP(x, y) (((x) / (y) + (int)((x) % (y) > 0)) * (y))
#endif

// 先用一个origin_ram申请一块足够大的片上空间
// 后续算子所需的各个具体空间的初始化可以通过各自加偏移值来实现
__nram__ char nram_buffer[MAX_NRAM_SIZE];

// 封一个compute函数，会在入口函数中调用
// device代码中，使用__mlu_func__修饰一般函数
__mlu_func__ void compute(const float alpha,
                          const float *x,
                          const float *y,
                          const int32_t n,
                          float *output) {
  // 此处将nram划分为两块空间nram_x、nram_y，分别用于存放x、y
  // 由于计算指令需要对齐到128B，此处num_deal表示nram_x一次能存放多少个对齐后的数据
  const int32_t num_deal = PAD_DOWN(MAX_NRAM_SIZE / 2 / sizeof(float),
                                    NFU_ALIGN_SIZE / sizeof(float));
  char *nram_x = nram_buffer;
  char *nram_y = nram_buffer + MAX_NRAM_SIZE / 2;

  // 当前core需要处理的数据量为n，此处计算需重复处理的次数
  const int32_t repeat = n / num_deal;
  const int32_t remin = n % num_deal;

  // 循环处理部分
  for (int32_t i = 0; i < repeat; ++i) {
    // load
    __memcpy_async(nram_x, x + i * num_deal, num_deal * sizeof(float),
                   GDRAM2NRAM);
    __memcpy_async(nram_y, y + i * num_deal, num_deal * sizeof(float),
                   GDRAM2NRAM);
    // __asm__ volatile("sync;\n\t"); 对齐_async指令
    __asm__ volatile("sync;\n\t");

    // compute
    __bang_add((float *)nram_y, (float *)nram_y, (float *)nram_x, num_deal);
    __bang_add_scalar((float *)nram_y, (float *)nram_y, alpha, num_deal);
    __asm__ volatile("sync;\n\t");

    // store
    __memcpy_async(output + i * num_deal, nram_y, num_deal * sizeof(float),
                   NRAM2GDRAM);
    __asm__ volatile("sync;\n\t");
  }

  // 处理余数部分
  if (remin > 0) {
    const int32_t remin_align = PAD_UP(remin, NFU_ALIGN_SIZE / sizeof(float));

    __memcpy_async(nram_x, x + repeat * num_deal,
                   remin * sizeof(float), GDRAM2NRAM);
    __memcpy_async(nram_y, y + repeat * num_deal,
                   remin * sizeof(float), GDRAM2NRAM);
    __asm__ volatile("sync;\n\t");

    __bang_add((float *)nram_y, (float *)nram_y, (float *)nram_x, remin_align);
    __bang_add_scalar((float *)nram_y, (float *)nram_y, alpha, remin_align);
    __asm__ volatile("sync;\n\t");

    __memcpy_async(output + repeat * num_deal, nram_y,
                   remin * sizeof(float), NRAM2GDRAM);
    __asm__ volatile("sync;\n\t");
  }
}

// kernel入口函数
__mlu_global__ void MLUKernelAdd(const float *x,
                                 const float *y,
                                 const int32_t N,
                                 const float alpha,
                                 float *output) {
  /* 在每个core运行时，我们可以通过taskDim和taskId获取有关多核的信息。
      - taskDim可以表示执行该算子时触发的core总数
      - taskId可以表示执行时这个core对应的序号，taskId ∈[0, taskDim - 1]
       
      有了这两个信息，
      我们便可以计算出每个core需要处理的数据大小、每个core对输入输出空间
      访存时加的偏移各是多少等等。实际的拆分策略还是需要根据算子实际情况
      来考虑。比如有的算子得在最高维度拆分，有的算子只需对所有数据平均拆分。 */

  // 平均每个核要处理的数据
  const int32_t n_seg = N / taskDim + (taskId == taskDim - 1) * (N % taskDim);

  // 每个core要对不同的数据区域做处理，所以需要根据taskId加偏移
  const float *x_offset = x + N / taskDim * taskId;
  const float *y_offset = y + N / taskDim * taskId;
  float *output_offset = output + N / taskDim * taskId;

  // 调用实现函数
  compute(alpha, x_offset, y_offset, n_seg, output_offset);
}

void MLUOP_WIN_API
KernelAdd(const cnrtDim3_t k_dim, const cnrtFunctionType_t k_type,
          const cnrtQueue_t queue, const float *x,
          const float *y, const int32_t element_num,
          const float alpha, float *output) {
  MLUKernelAdd<<<k_dim, k_type, queue>>>(
    x, y, element_num, alpha, output);
}