/*************************************************************************
 * Copyright (C) [2023] by Cambricon, Inc.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the
 * "Software"), to deal in the Software without restriction, including
 * without limitation the rights to use, copy, modify, merge, publish,
 * distribute, sublicense, and/or sell copies of the Software, and to
 * permit persons to whom the Software is furnished to do so, subject to
 * the following conditions:
 *
 * The above copyright notice and this permission notice shall be included
 * in all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
 * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
 * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 *************************************************************************/
#include "dot_product.h"
#include "kernels/fill_zero/fill_zero.h"

#include "kernels/kernel.h"
#include "kernels/debug.h"
#include "core/logging.h"

#ifndef PAD_DOWN
#define PAD_DOWN(x, y) (((x) / (y)) * (y))
#endif

#ifndef PAD_UP
#define PAD_UP(x, y) (((x) / (y) + (int)((x) % (y) > 0)) * (y))
#endif

// 先用一个origin_ram申请一块足够大的片上空间
// 后续算子所需的各个具体空间的初始化可以通过各自加偏移值来实现
__nram__ char nram_buffer[MAX_NRAM_SIZE];
__nram__ char nram_buffer2[128];

template<typename T>
__mlu_func__ void compute(const T *x, const T *y, 
                        const int32_t n, T *output) {
  // 此处将nram划分为两块空间nram_x、nram_y
  // 由于计算指令需要对齐到128B，此处num_deal表示nram_x一次能存放多少个对齐后的数据
  const int32_t num_deal = PAD_DOWN(MAX_NRAM_SIZE / 2,
                                    NFU_ALIGN_SIZE) / sizeof(T);
  char *nram_x = nram_buffer;
  char *nram_y = nram_buffer + MAX_NRAM_SIZE / 2;
  
  // 当前core需要处理的数据量为n，此处计算需重复处理的次数
  const int32_t repeat = n / num_deal;
  const int32_t remin = n % num_deal;

  //__bang_sumpool需要的参数
  const int32_t channel = NFU_ALIGN_SIZE / sizeof(T);
  const int32_t width = num_deal / channel;

  // 循环处理部分
  for (int32_t i = 0; i < repeat; ++i) {
    // load
    __memcpy_async(nram_x, x + i * num_deal, num_deal * sizeof(T),
                   GDRAM2NRAM);
    __memcpy_async(nram_y, y + i * num_deal, num_deal * sizeof(T),
                   GDRAM2NRAM);
    // __asm__ volatile("sync;\n\t"); 对齐_async指令
    __sync();
    // compute
    // 对应位数乘
    __bang_mul((T *)nram_x, (T *)nram_x, (T *)nram_y, num_deal);
    // 将值累加到第一位
    //
    __bang_sumpool((T *)nram_x, (T *)nram_x, channel, 1,
                   width, 1, width, 1, width);
    
    __bang_reduce_sum((T *)nram_x, (T *)nram_x, channel);
    // 加到缓冲区
    T a;
    a += *(T *)nram_x;
    __bang_add
    __bang_atomic_reduce_add(output, (T *)nram_x, 1);
  }

  // 处理余数部分
  if (remin > 0) {
    __memcpy_async(nram_x, x + repeat * num_deal,
                   remin * sizeof(T), GDRAM2NRAM);
    __memcpy_async(nram_y, y + repeat * num_deal,
                   remin * sizeof(T), GDRAM2NRAM);
    // remin以128字节向上取整
    const int32_t remin_align = PAD_UP(remin, channel);
    // 如果remin * sizeof(T)不是128的倍数，则需要处理尾数,通过补0处理
    if (remin_align > remin) {
      T pad = 0;
      __memset_nram_async((T *)nram_x + remin, remin_align - remin, pad);
      __memset_nram_async((T *)nram_y + remin, remin_align - remin, pad);
    }
    __sync();

    // compute
    __bang_mul((T *)nram_x, (T *)nram_x, (T *)nram_y, remin_align);
    // 将值累加到第一位
    const int32_t remin_width = remin_align / channel;
    //
    __bang_sumpool((T *)nram_x, (T *)nram_x, channel, 1,
                   remin_width, 1, remin_width, 1, remin_width);
    // __bang_reduce_sum按128字节求和放置首位
    __bang_reduce_sum((T *)nram_x, (T *)nram_x, channel);
    // 加到缓冲区
    __bang_atomic_reduce_add(output, (T *)nram_x, 1);
  }

}

// kernel入口函数
template<typename T>
__mlu_global__ void MLUKernelDotProduct(const T *x,
                                        const T *y,
                                        const int32_t element_num,
                                        T *output) {
  /* 在每个core运行时，我们可以通过taskDim和taskId获取有关多核的信息。
      - taskDim可以表示执行该算子时触发的core总数
      - taskId可以表示执行时这个core对应的序号，taskId ∈[0, tullptraskDim - 1]
      
      有了这两个信息，
      我们便可以计算出每个core需要处理的数据大小、每个core对输入输出空间
      访存时加的偏移各是多少等等。实际的拆分策略还是需要根据算子实际情况
      来考虑。比如有的算子得在最高维度拆分，有的算子只需对所有数据平均拆分。 */

  // 平均每个核要处理的数据
  const int32_t n_seg = element_num / taskDim + (taskId == taskDim - 1)
                       * (element_num % taskDim);

  // 每个core要对不同的数据区域做处理，所以需要根据taskId加偏移
  const T *x_offset = x + element_num / taskDim * taskId;
  const T *y_offset = y + element_num / taskDim * taskId;

  // 调用实现函数
  compute(x_offset, y_offset, n_seg, output);
}

mluOpStatus_t MLUOP_WIN_API
KernelDotProduct(const cnrtDim3_t k_dim, const cnrtFunctionType_t k_type,
                 const cnrtQueue_t queue, const mluOpDataType_t d_type,
                 const void *x, const void *y, const int32_t element_num,
                 void *output) {
  // 调用KernelFillZero算子赋0值
  cnrtDim3_t tmp_k_dim;
  tmp_k_dim.x = 1;
  tmp_k_dim.y = 1;
  tmp_k_dim.z = 1;
  cnrtFunctionType_t tmp_k_type;
  tmp_k_type = CNRT_FUNC_TYPE_BLOCK;
  // 根据类型选择
  switch (d_type) {
    case MLUOP_DTYPE_FLOAT: {
      // 赋0
      KernelFillZero(k_dim, k_type, queue, sizeof(float), output);
      // float操作
      MLUKernelDotProduct<<<k_dim, k_type, queue>>>((float*)x, 
      (float*)y, element_num, (float*)output);
    }; break;
    case MLUOP_DTYPE_HALF: {
      // 赋0
      KernelFillZero(k_dim, k_type, queue, sizeof(half), output);
      // half操作
      MLUKernelDotProduct<<<k_dim, k_type, queue>>>((half*)x, 
      (half*)y, element_num, (half*)output);
    }; break;
    default: {
      MLULOG(ERROR) << "Not implemented.";
    }; break;
  }
  return MLUOP_STATUS_SUCCESS;
}
